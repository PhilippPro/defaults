---
title: "Multiple Defaults Search Output"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
loadd(learner.name)
loadd(data)
loadd(data_aggfun)
loadd(p)
loadd(g)
loadd(r)
loadd(rankplot)
loadd(nfold)
loadd(tsneplot)
```



# `r learner.name`

All plots indicate results from the learner `r learner.name`.

**Current status: `r round(sum(data$cnt) / (nrow(data) * 36), 3) * 100` \% of results finished.**

### Caveats

- I can create the same plots for Accuracy, and F1 without any big additional effort if required.

- The current analysis contains 36 / 38 datasets for which we have data from the randomBot.
  I left out two datasets, because they take really long to evaluate (because they are very big).

- The general process of selecting a best configuration using nested CV seems to be decently unstable, at 
  least in the decision trees, we sometimes get worse by selecting a bigger subset of the learned defaults.
  This is an interesting resarch subject in itself, but might influence our analysis.


## Average rank 

The following table contains the average rank, mean auc, normalized mean auc and median of auc across
n = 36 hold-out datasets. 
All results have been obtained using 10-fold CV splits from OpenML and 5-fold nested CV for 
selecting a configuration in the multiple defaults and random-search case.

Attention has been given to the **n** column, as $n < 36$ indicates missing results

```{r, echo=FALSE}
print(data)
```


## Comparison to randomSearch

Boxplot of n found defaults (design) vs. package defaults and random search with design.

**Left** Boxplot of performances using n = 2, 4, 6, 8, 10 complementary defaults

**Mid** Package Defaults

**Right** Randomsearch with n = 4, 8, 16, 32, 64 configurations randomly drawn from the parameter space.


```{r, echo = FALSE}
p
```


## Comparison to n-fold randomSearch

Boxplot of n (2, 4, 8) found defaults (design) vs. random search with $n \cdot 1,2,4,8$ times random search.
Values $> 0$ indicate an improvement of defaults over randomSearch.

```{r, echo = FALSE}
nfold
```

## Comparison to randomBotdata

Boxplot of minimum over n = 4, 8, 16, 32, 64 randomly sampled configurations from the randomBotdata.

```{r, echo = FALSE}
r
```


## Delta comparison to package defaults
This plot indicates the difference to package defaults measured in AUC.
Values larger than 0 indicate improvement over package defaults.

```{r, echo = FALSE, warning=FALSE}
g
```


## Plotting the ranks

Plot containing the ranks of the different elements of the first table
```{r, echo = FALSE, warning = FALSE}
rankplot
```

## Comparing different Aggregation functions:

**design**: Aggregate using the median

**default_mean**: Aggregate using the mean

**default_cycle**: Aggregate cycling through the quantiles (0.7, 0.3 and 0.5)

If the following table does not contain differnt **search.types**, this indicates that results have not yet been computed for this learner.

*mn_auc_norm.* is short for mean_auc_normalized (normalized to [0, 1]).

```{r, echo = FALSE}
data_aggfun
```


## TSNE Plot

Plot breaking down the parameter space to two dimensions using TSNE. 
The different iters refer to different Leave-One-Out Splits of the dataset.

```{r, echo = FALSE}
tsneplot
```