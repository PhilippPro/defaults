# Additional Insights

```{r, eval = FALSE}
# Loading required packages.
library(devtools)
load_all()
```

This notebook covers some of the design choices made
during the development.

## Aggregation Functions

We compare three possible aggregation functions:
**mean**, **median**, **mix**.
**mix** is defined as $0.5 * mean + 0.5 * median$.

Note: The default option "mix" can be found in
`02_calculate_defaults.R`.

```{r}
# mean vs median: mean works better (by ~0.5%)
sc_xgb = make_surrogates_omlbot(baselearners = "xgboost", measures = measures)
res_xgb_median = foreach(oml_task_id = oml_task_ids, .combine = "cbind") %dopar% {
  # Search  Defaults, hold out task x
  ds = DefaultSearch$new(sc_xgb, n_defaults, oml_task_id, "median")
  ds$search_defaults()
  ds$save_to_disk()
  ds$get_holdout_performance()
}

# additionally test "mix" (mean + median) / 2: mix is better!
sc_xgb = make_surrogates_omlbot(baselearners = "xgboost", measures = measures)
res_xgb_mean = foreach(oml_task_id = oml_task_ids, .combine = "cbind") %dopar% {
  # Search  Defaults, hold out task x
  ds = DefaultSearch$new(sc_xgb, n_defaults, oml_task_id, "mean")
  ds$search_defaults()
  ds$save_to_disk()
  ds$get_holdout_performance()
}
```

And we visualize the results:

```{r}
xgb_aggfuns = list(
  "xgb_mean" = res_xgb_mean,
  "xgb_median" = res_xgb_median,
  "xgb_mix"  = res_xgb)

rbind_res(xgb_aggfuns) %>% spread(method, auc) %>% group_by(iter) %>% summarize(d_med = mean(xgb_median - xgb_mean), d_mix = mean(xgb_mix - xgb_mean), vs_med = mean(xgb_mean > xgb_median), vs_mix = mean(xgb_mean > xgb_mix), med_vs_mean = mean(xgb_median > xgb_mix))
plot_res(xgb_aggfuns)
```


Conclusions:
- mix > mean > median
- For higher n, median becomes better, mix is still the
  best for n = 16
=> We will use "mix" as a default.


## Random Search: Number of points

The number of random search evals massively influences
computation time for defaults.
We parallelize across 12 cores and do leave-one-dataset-out CV.
With 10k, finding 16 defaults takes ~ 21 minutes.
For 100k: 210 minutes = ~ 3.5 hours.


```{r}
  library(tictoc) # Measure time
  tic()
  ds = DefaultSearch$new(sc_xgb, 16, NULL, "mix", learner_prefix = "full")
  ds$search_defaults()
  toc()
```

=> Finding 16 defaults on 38 datasets takes around
   7.5 minutes using $10^4$ points on a single core.


### Experiment 1

We compare 10^4 vs 10^5 points using the mean as an
aggregation function.

```{r}
sc_xgb = make_surrogates_omlbot(baselearners = "xgboost", measures = measures)
res_xgb_100k = foreach(oml_task_id = oml_task_ids, .combine = "cbind") %dopar% {
  # Search  Defaults, hold out task x
  ds = DefaultSearch$new(sc_xgb, 8, oml_task_id, "mean", learner_prefix = "100k")
  ds$ctrl$points = 10^5
  ds$search_defaults()
  ds$save_to_disk()
  ds$get_holdout_performance()[1:8, , drop = FALSE]
}
```

```{r}
xgb_10k_vs_100k = list(
  "k100" = res_xgb_100k,
  "k10" = res_xgb_mean)

rbind_res(xgb_10k_vs_100k) %>%
  spread(method, auc) %>%
  group_by(iter) %>%
  summarize(mean(k100 - k10), mean(k100 > k10))
```


### Experiment 2

We compare 10k vs 20k points using "mix" aggregation function.

```{r}
sc_xgb = make_surrogates_omlbot(baselearners = "xgboost", measures = measures)
res_xgb_20k = foreach(oml_task_id = oml_task_ids, .combine = "cbind") %dopar% {
  # Search  Defaults, hold out task x
  ds = DefaultSearch$new(sc_xgb, n_defaults, oml_task_id, "mix", learner_prefix = "20k")
  ds$ctrl$points = 2*10^4
  ds$search_defaults()
  ds$save_to_disk()
  ds$get_holdout_performance()
}
```

```{r}
sc_xgb = make_surrogates_omlbot(baselearners = "xgboost", measures = measures)
res_xgb_1k = foreach(oml_task_id = oml_task_ids, .combine = "cbind") %dopar% {
  # Search  Defaults, hold out task x
  ds = DefaultSearch$new(sc_xgb, n_defaults, oml_task_id, "mix", learner_prefix = "1k")
  ds$ctrl$points = 10^3
  ds$search_defaults()
  ds$save_to_disk()
  ds$get_holdout_performance()
}
```


```{r}
xgb_10k_vs_20k = list(
  "k20" = res_xgb_20k,
  "k10" = res_xgb,
  "k1" = res_xgb_1k)

rbind_res(xgb_10k_vs_20k) %>%
  spread(method, auc) %>%
  group_by(iter) %>%
  summarize(mean(k20 - k10), mean(k20 > k10), mean(k1 > k10))
```

Conclusion: Higher number of points is favourable, 10k seems about as good as 20k though.

## Performance versus Runtime


We want to trade-off performance and runtime.
We currently use the following formula:

$acc_{scaled} / log(rt_{scaled} + base, base = base)^{power}$

```{r}
registerDoParallel(10)
# Speedup: Only on 5 datasets
sc_xgb = make_surrogates_omlbot(baselearners = "xgboost",
  measures = "auc")
sc_xgb$surrogates[6:38] = NULL

res = foreach (power = c(0.1, 1, 2)) %:%
  foreach(base = c(1.01, exp(1), 10)) %do% {
  sct = make_surrogates_omlbot(get_oml_task_ids()[1:5],
    baselearners = "xgboost", measures = "auc",
    data_source = "data/input/oml_bot_xgb_sample.RDS",
    scaler = ScalerTimeCrit$new(base = base, power = power))
  foreach(oml_task_id = get_oml_task_ids()[1:5], .combine = "cbind") %do% {
    # Search  Defaults, hold out task x
    ds = DefaultSearch$new(sct, 4, oml_task_id)
    ds$ctrl$points = 200
    ds$search_defaults()
    get_holdout_perf(sc_xgb, ds$defaults.params, oml_task_id)
}}

agg_mean = lapply(res, function(x) sapply(x, function(y) {apply(apply(y, 2, cummax), 1, mean)}))
agg_mean = sapply(agg_mean, function(x) x[4,])
colnames(agg_mean) = c(0.1, 1, 2)
rownames(agg_mean) = c(1.01, exp(1), 10)


# Results (The following seems to work well)
# Base 1.01 Power 0.1
#      10   Power 0.1
#      e    Power 1

res = foreach (power = c(0.01, 0.1, 0.5, 1, 2)) %:%
  foreach(oml_task_id = get_oml_task_ids()[1:5], .combine = "cbind") %do% {
    sct = make_surrogates_omlbot(get_oml_task_ids()[1:5],
    baselearners = "xgboost", measures = "auc",
    data_source = "data/input/oml_bot_xgb_sample.RDS",
    scaler = ScalerTimeCrit$new(base = 1, power = power))
    # Search  Defaults, hold out task x
    ds = DefaultSearch$new(sct, 4, oml_task_id)
    ds$ctrl$points = 1000
    ds$search_defaults()
    get_holdout_perf(sc_xgb, ds$defaults.params, oml_task_id)
}
lapply(res, function(x) apply(x, 1, mean))
```



```{r, eval = FALSE}
df = readRDS("data/input/oml_bot_data.RDS")
```

```{r}
# Performance versus Runtime
library(dplyr)
library(ggplot2)
task.id = 9952
p = df %>%
  mutate(color = if_else(is.na(kernel),
    if_else(!is.na(booster),
      paste0("booster:", as.character(booster)),
      if_else(num.trees > 1000, ">1000 trees", "<1000 trees")
    ),
    paste0("kernel:" , as.character(kernel)))
  ) %>%
  mutate(color = as.factor(color)) %>%
  filter(task_id == task.id) %>%
  filter(learner_id %in% c("mlr.classif.xgboost", "mlr.classif.ranger", "mlr.classif.svm")) %>%
  ggplot() +
    geom_point(aes(x = performance, y = runtime, color = color), alpha = 0.3) +
    facet_grid(learner_id ~ measure, scales = "free") +
    scale_y_log10()
# ggsave(p, filename = paste0("../../perf_vs_runtime_", task.id), device = "jpg")
p
```

## Performance scaled by Runtime

```{r}
# Performance versus Runtime
library(dplyr)
library(ggplot2)
task.id = 9952
p = df %>%
  mutate(color = if_else(is.na(kernel),
    if_else(!is.na(booster),
      paste0("booster:", as.character(booster)),
      if_else(num.trees > 1000, ">1000 trees", "<1000 trees")
    ),
    paste0("kernel:" , as.character(kernel)))
  ) %>%
  mutate(color = as.factor(color)) %>%
  filter(task_id == task.id) %>%
  filter(learner_id %in% c("mlr.classif.xgboost", "mlr.classif.ranger", "mlr.classif.svm")) %>%
  group_by(learner_id, measure) %>%
  filter(measure != "brier") %>%
  mutate(performance_normalized = BBmisc::normalize(performance, "range")) %>%
  ungroup() %>%
  mutate(performance_rt = performance_normalized / log(runtime + 10, base = 10)^0.15) %>%
  ggplot() +
    geom_point(aes(x = performance_rt, y = performance_normalized, color = color), alpha = 0.3) +
    facet_grid(learner_id ~ measure, scales = "free")
ggsave(p, filename = paste0("../../perf_vs_runtime_logscaled", task.id), device = "jpg")
p
```


## Performance without surrogates (in sample)

This section tries to explore the trade-off between runtime and performance in an overly optimistic manner:
We compute a performance criterion (see comments below) and look for a point that optimizes the given criterion in-sample.
We then compare to the fastest configuration that achieved the maximum AUC.

```{r}
df = readRDS("data/input/oml_bot_xgb_sample.RDS")
# Convert to data.table for speedup
dt = as.data.table(df)[!is.na(performance) & !is.na(runtime) , .(runtime, performance, measure, learner_id, task_id)]



# We use formulae of the form performance / log(runtime + base, base = base)^pow i.e.
# we scale the denominator (runtime).

dtout = data.table()
for (base in c(1.001, 1.01, 2, exp(1), 10)) {
for (power in c(0.1, 0.5, 1, 2)) {

  dt2 = dt[, .(perf = BBmisc::normalize(performance, method = "range"), runtime = runtime/max(runtime)), by = .(task_id)]
  dt2 = dt2[, .(perf, runtime, task_id, perf_by_time = perf / log(runtime + base, base = base)^power)]


  dt3 = merge(
    dt2[, .(perf, runtime, perf_by_time, pbt = max(perf_by_time)), by = .(task_id)][perf_by_time == pbt, ],
    dt2[, .(perf, runtime, pm = max(perf)), by = .(task_id)][perf == pm,],
    by = "task_id"
  )

  dtn = merge(
    dt3[, .(runtime_pct = runtime.x / runtime.y, perf_pct = perf.x / perf.y, task_id)],
    dt3[, .(max_runtime_pct = max(runtime.x / runtime.y)), by = "task_id"],
    by = "task_id"
  )
  dtout = rbind(dtout, dtn[max_runtime_pct == runtime_pct,][, .(perf_pct = mean(perf_pct), runtime_pct = mean(runtime_pct), base = base, power = power)])
  }
}

p = ggplot(dtout, aes(perf_pct, runtime_pct)) + geom_point()  + geom_line() + xlab("Percent of max auc") + ylab("Percent of runtime to achieve best auc")
ggsave(p,  file = "../perf_vs_runtime_rpart.pdf")

# In-sample result:
# bs:10, power: 0.1
#    perf_pct runtime_pct
#    0.9905      0.1046
# => We can get 99.05% of average performance for 10.46% of computation time
# bs: 10, power: 1
#   perf_pct runtime_pct
#    0.9643     0.08621
# => We can get 96.43% of avg performance for 8.6% of computation time



dtout = data.table()
for (power in c(0.001, 0.01, 0.1, 0.5, 1, 2)) {

  dt2 = dt[, .(perf = BBmisc::normalize(performance, method = "range"), runtime = runtime/max(runtime)), by = .(task_id)]
  dt2 = dt2[, .(perf, runtime, task_id, perf_by_time = perf / (runtime^power))]


  dt3 = merge(
    dt2[, .(perf, runtime, perf_by_time, pbt = max(perf_by_time)), by = .(task_id)][perf_by_time == pbt, ],
    dt2[, .(perf, runtime, pm = max(perf)), by = .(task_id)][perf == pm,],
    by = "task_id"
  )

  dtn = merge(
    dt3[, .(runtime_pct = runtime.x / runtime.y, perf_pct = perf.x / perf.y, task_id)],
    dt3[, .(max_runtime_pct = max(runtime.x / runtime.y)), by = "task_id"],
    by = "task_id"
  )
  dtout = rbind(dtout, dtn[max_runtime_pct == runtime_pct,][, .(perf_pct = mean(perf_pct), runtime_pct = mean(runtime_pct), power = power)])
}
dtout

p = ggplot(dtout, aes(perf_pct, runtime_pct)) + geom_point()  + geom_line() + xlab("Percent of max auc") + ylab("Percent of runtime to achieve best auc")
ggsave(p,  file = "../perf_vs_runtime_rpart.pdf")
```


## Creating performance ranges

```{r}
ranges = get_ranges_multi_baselearners("data/input/oml_bot_data.RDS", get_mlr_baselearners(), get_measures(), get_oml_task_ids())
saveRDS(lapply(ranges, as.data.frame), "data/input/oml_mlr_ranges.RDS")
# Add runtime
d = readRDS("data/input/oml_bot_data.RDS")
d = d %>% dplyr::filter(!is.na(performance)) %>% dplyr::ungroup()
ranges = d %>%
    dplyr::filter(task_id %in% get_oml_task_ids()) %>%
    dplyr::group_by(task_id, measure) %>%
    dplyr::mutate(runtime = runtime + abs(min(0, runtime)) + 1) %>%
    dplyr::filter(measure == "acc") %>%
    dplyr::summarise(min = min(runtime), max = max(runtime)) %>%
    dplyr::select(-measure) %>%
    as.data.frame()

dd = readRDS("data/input/oml_mlr_ranges.RDS")
dd[["runtime"]] = ranges
saveRDS(dd, "data/input/oml_mlr_ranges.RDS")



library(dplyr)
d %>% dplyr::filter(learner_id == "mlr.classif.xgboost") %>%
 filter(task_id %in% get_oml_task_ids()) %>% group_by(task_id) %>% tally() %>% data.frame()
get

```
