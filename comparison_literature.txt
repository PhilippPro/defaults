
(A) Wistuba Learning Hyperparameter Optimization Initializations (2015)
Strategy:
  - Optimize a differentiable (via softmin) approximation to a meta-loss  (Different)
  - Scale performance to 0-1 on each dataset (We sort of show that this does not really matter)
  - Loss is then the minimum over set of configs averaged over all datasets (equal to ours)
  - Add dataset similarity as weights. (Conclusion: Reweighting does not show better results)
  - Greedy approach, but with respect to weights and adpating to earlier defaults found on held-out data (Not applicable in our setting)
  - Search carried out via Gradient Descent.
  - Differentiable meta loss requires a fully numeric parameter space. They one-hot encode the svm kernel hyperparameter.
  - Can not deal with hierarchies, (apparently factors), ...
Evaluation:
  - 50 Classification data sets
  - 80/20 Train/Test
  - Leave-one-Dataset-Out CV
  - SVM (3 Kernels) and Adaboost (kernel parameter is one-hot encoded)



(B) Sequential Model-free Hyperparameter Tuning (2015)
Strategy:
  - Model-free and meta-feature free hyperparameter tuning strategy
  - Scale performance to 0-1 on each dataset
  - Greedy forward search on a grid, but minimize the sum-of-ranks.
  - They provide a set of parameters: http://www.hylap.org/publications/Sequential-model-free-hyperparameter-tuning
Evaluation:
  - 25 Classification data sets
  - 80/20 Train/Test split
  - SVM (3 Kernels) and Adaboost (kernel parameter is one-hot encoded)


General Differences:
  To Both:
    - Evaluation is unrealistic, selecting lambda \in Lambda* leads to model selection bias.
      This is critical especially on small datasets.
    - We compare across 6 algorithms, bigger search space.
    - We use surrogates, the do not.
    - Apparently, the approach and strategy is used in many papers:
      - Feuerer at al. 2015 (Using meta-learning to initialize BO of hyperparameters)
      - Reif et al. 2012 (Meta-learning for evolutionary parameter optimization of classifiers)
      - Both Wistuba Papers (A, B)

  To A:
    - Grad Descent on meta-loss, starting from good grid points
      instead of random search on surrogates.
    - How it deals with factors is unclear/hacky
    - How it deals with hierarchies is unclear/hacky
    - Can not provide a list of parameters.

  To B:
    - We use surrogates, are thus not constrained to grid.
    - Can only contain solutions from the grid.
    - We scale with dimensions, grid search hardly does.
    - In theory their method would also work on a dataset initiated with random search.








